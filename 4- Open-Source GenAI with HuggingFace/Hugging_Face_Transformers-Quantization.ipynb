{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOg9BwWpu5Cg"
   },
   "source": [
    "## Models\n",
    "\n",
    "- Use the low-level Transformers API to access model classes (they wrap the underlying PyTorch code).  \n",
    "- We can run this notebook on a free or low-cost T4 GPU runtime.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Scfe0epuS9Z",
    "outputId": "f345da49-0096-4fbe-d7d5-9b2c2e5d206e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pQvDf36eu56e"
   },
   "outputs": [],
   "source": [
    "import gc    # garbage collector\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHC-Ww0dwovG"
   },
   "source": [
    "## 1. Hugging Face API Token\n",
    "\n",
    "1. Go to https://huggingface.co and **sign up** or log in.  \n",
    "2. Open **Settings → Access Tokens** and click **Create new token**.  \n",
    "3. Under **Permissions**, select **Read & Write**, then **Generate** and copy the token.  \n",
    "4. Press the \"key\" icon in your side-panel on the left, add a secret:  \n",
    "   ```bash\n",
    "   HF_TOKEN=<your_token>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6vGsXB-_u583"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get(\"HF_TOKEN\")\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lPVsAtD3u5_T"
   },
   "outputs": [],
   "source": [
    "# Instruct models\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA2 = \"google/gemma-2-2b-it\"\n",
    "QWEN2 = \"Qwen/Qwen2-7B-Instruct\"\n",
    "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KFi8j-znu6Bj"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of AI Engineer\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNtvGC2xrua7"
   },
   "source": [
    "## 2- Quantization\n",
    "\n",
    "- **What?**  \n",
    "  Reduces the bit-width of model weights (normally 32-bit floats) to smaller sizes (8-bit, 4-bit).  \n",
    "\n",
    "  Accuracy sure is hurt, but not by as much as we might expect.\n",
    "- **Why?**  \n",
    "  • Saves GPU/CPU memory  \n",
    "  • Speeds up inference  \n",
    "  • Enables running large models on smaller hardware  \n",
    "- **How?**  \n",
    "  Load the model with a quantization config instead of full precision.\n",
    "\n",
    "  Now, we access Llama 3.1 from Meta.\n",
    "\n",
    "\n",
    "\n",
    "  ### Special Tokens: EOS & PAD\n",
    "\n",
    "- **`eos_token`**  \n",
    "  Marks the **end of a sequence** (where generation stops).\n",
    "\n",
    "- **`pad_token`**  \n",
    "  Fills shorter sequences up to a fixed length for batching.  \n",
    "  By setting it to `eos_token`, you treat padding as “end-of-sequence” and avoid warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419,
     "referenced_widgets": [
      "14648aa1c7164643a18d3fcb1a2ceaa9",
      "d633ea80c9084ee9ba287246c7f9dab5",
      "836319f59eb14733bfb93bf3136de3b4",
      "bea7aab0eb89473baeba674929ce1336",
      "34939669245e4f7ea598c0688fe7a3f1",
      "e5e1e2e84a7542389813c6557148c958",
      "35c57652c7fc49eb9941a8938f758810",
      "4353c8be06144136ad963ee396ffbc40",
      "c42843e70c5e4c8bb1314978881caf19",
      "3ded99a0bd5548cfa69b752a7040250e",
      "ef36c563cf694f4983a156f515fb987a",
      "ebd1f285e8a343b9add32a5a9d8e5432",
      "029903efd221408197443960da2a735a",
      "9a8283c5540d47a391dc637042b1e567",
      "1353572df7fc4871b788a5c820d8ed55",
      "28be1db5368e4535a8b8c8223414411f",
      "18c17b70e7f742ef9384d49577fb8fbc",
      "512f98b03b574d519f136d583f832688",
      "810fd7ccaa7646e8a53391f12f0e0503",
      "5c5bdff4f6ac4873a2c9d2d169cb9ad0",
      "299d2658085a4ebaa435ef3c2dee86c1",
      "ddcc3d3e684544fa907a87d121902803",
      "094954e419b84f308489e3294ee7a0dd",
      "b96d62ffc11b4158aa63f34b3597fa9e",
      "ee258f2803824236b59b3f2b26ab5917",
      "dbb962092bac4d81825b6ee96f3390a5",
      "96c0fda4c2114690884b5f78373c9a4b",
      "8c84657081e248088f7dc00bc33d3486",
      "109eac2225e94354a485dfb9b34b3c54",
      "39c13afd355a4506bf2b262526436d47",
      "239f854d645443e797c70db8d6cc5f79",
      "7fb5646ab7f742a59ecf641d0979bff6",
      "508d5f2b066c4d0487b247fa4b139053",
      "e6d33852f4374936b46f5dcd2c4ea674",
      "1fe7799c49704e52aba0e3f0bad97008",
      "ea9a329a0b3a4aecb4575fa821cc036e",
      "cf56359bf8a44766b6e9465f457e9942",
      "5d7a54d3bd504c0d9d089a7f14bbc887",
      "3453bee48ea74501816aacc0f5594a48",
      "6887853cc5004e4897a0e8e2f6db93ad",
      "d561a51f66674a4386a345ca03d3f728",
      "7200217a6a014f01b9c32b414108d514",
      "e0c41b95cd2a4746934837f4e3139e11",
      "4765c2f4ca0f4b5d912030dffdb4d43c",
      "a8e7769ede4441d59ca8d74b53009f32",
      "d9e546ea7c2645b8b530a7c85fba6b2a",
      "6937ee9b49d1436b9014de71320f64fc",
      "eb5ee03bec524e3992a124f09bd440b9",
      "d7354037a641499f84847cd05a9bf0a5",
      "6ceafdc6c3134061b7f32b633a051b26",
      "a5825f45440d48029c7f21efe3b421fe",
      "25ef3f7217db4dce9d5ff36f263fb831",
      "f022c69ff5494b07ba11eaa94353b817",
      "b7af5bbe916949a9914197fdb8c99f85",
      "d9e2bdc057f7431b857a1bc402221ba0",
      "e51b0d63f2724a559695128604c4b334",
      "da29e08670c14afaa44568c71ca9732b",
      "088434be2e9d499eb3247de3e8cc125c",
      "b1de5b94a90945efb4c4dd597c95cb14",
      "5f7561fca4714e20903311a09cc7553e",
      "0c2a8a7e29c34c49b6c3f36e5155ed69",
      "5bd0662cb8f14bf8b4c6d2bdd25a2e8e",
      "456592f80d8541c4b64cd298964ec4e2",
      "a99ec7cc8e86448fa03c4ae2d13f8c4d",
      "1c98ff5db9b549f3a208fd28d6fd2b0f",
      "fafe87c9bf7c4cb38971a8efa732818e",
      "12b3aa7cfe604fd9bd281f683d83f846",
      "778911f29c984e5dba056fabe1524fa2",
      "923870535a5b483582989d3667c96e53",
      "9d17a11f279f4d69bcb815645d6d4ea4",
      "67746e2796d44898a5ab197a656972cf",
      "9e228f8e719847bc901d9c13192741f8",
      "7d1a2f04b89540bbb9688a6b84f8a478",
      "d0a44ef8e3d34048bc27532826a82980",
      "1358e64b51c94d25a2c3366b76cd1cb4",
      "7d1f07ad654849d4836b9df627dd2a75",
      "bd8dc3d9d9d44134a3a46116560e8e42",
      "2d6e8c3a350e46d29ac1933cd288c168",
      "aec5af8f485d4ee68c43dc125e08d649",
      "c0fe72ce5b74459090887767e9a57037",
      "f00b21b9bf324746852f0e4a1c5cfc65",
      "38053396f6fa43a090d66f73e4c5f4fb",
      "5ad832ccde1941fbb4dd716a063b81d7",
      "eea68579ee204e4e88a20c18aff0aff7",
      "a8aa6b15502d4fce87ba747dd4f3ff46",
      "12830a1d1a7c4202ac7b8a634b8f983f",
      "890933858789463e83122d16b8e32338",
      "7905ada69af043d0863b296c9b6a3ed6",
      "0d6c2526b4a24691af21f483db69cc6f",
      "16f71052bf6e489dbf0eeb286ead4420",
      "d39b163d8eed4874ad7a266a9ef142d5",
      "328149643d9e43fea34ee31e77bf34be",
      "fa6e120d2c18433ea5a7c49187c24ed6",
      "2b0b979ecbc74447ae475b0c99db46e4",
      "6ad7be3a0fed4e36b1ebcb730cd574f0",
      "b0ffa0f437f6452fa736e58abe3d85d2",
      "fea31309a864421e98afdc93cae7f743",
      "a6ad5ebd98e9439d88cd16db4dbb50e6",
      "a38a16d705ad42d6afd55283f397326a",
      "ca0967287c4e4dc6a919f95e3e07fc5d",
      "f7195d49603541bcb87bb198a264bf06",
      "3b752b3002b145249fce4cb6f11f381e",
      "9a9562d51b3f440cb17b0272a2aa8225",
      "f422163e0a4a4a6aac47455e30902f41",
      "80d0ffb66c6942f3b684304fb671a346",
      "4e7d7d02c00f4eaf9aab4f604c2aeaad",
      "257b7aa2478a47dcbc03e45bdbd6fa13",
      "6d656cae84b44b56b29d1d9c0fe03f91",
      "68a2d2c8de2b4f84b6966d10c45832f0",
      "795009f564424869aead1686662cdabf",
      "8a8ee8f29ff2408985dda725842237f0",
      "c22c2a87093e4e85b50ef906b5183f6a",
      "f988ae1ece374b6aaa3b58248eabec64",
      "218a86afbef949e3b893805ab7bfc9d0",
      "329caded54754e37991c114d28206d12",
      "05447cd6216d4250955841587a808cdb",
      "f5152a0fd8b847a38208346a5468b196",
      "8a700089013e486dbae3170ff4481f14",
      "c2b7e794e9ee4ea293af8194ab3bad44",
      "fb7c6f1f057c4f44a2192d50a8db0980",
      "d242b0d37f044d91ad625048b85f9389",
      "76a111c9da1b45df8d7afe6b733436b8",
      "495b41a8340c4026bc2867bb6d078746",
      "03b4182434394d559b6cf7f31342892d",
      "812d8ddf1bd541df80de12893a037fd5",
      "3e979c5af3e94aaea05c32978768eb23",
      "cc546d9f10694a879e88b3d5a6ff89d4",
      "43359d40dd4c4e3d9bf9755e980795ae",
      "63df05b06c894af3ae476cf56c86569d",
      "31b8c937942f43b68e6e037b667cec7a",
      "c6cf9ad1a806498da97a42530f79783a",
      "5ceaf588aaee407294685fb37bb9908e"
     ]
    },
    "id": "vDUBgO8gu6EA",
    "outputId": "80a9f9d3-453d-470d-ff57-5701843a6fd2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14648aa1c7164643a18d3fcb1a2ceaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd1f285e8a343b9add32a5a9d8e5432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094954e419b84f308489e3294ee7a0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d33852f4374936b46f5dcd2c4ea674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e7769ede4441d59ca8d74b53009f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51b0d63f2724a559695128604c4b334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b3aa7cfe604fd9bd281f683d83f846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6e8c3a350e46d29ac1933cd288c168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6c2526b4a24691af21f483db69cc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0967287c4e4dc6a919f95e3e07fc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ee8f29ff2408985dda725842237f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a111c9da1b45df8d7afe6b733436b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "# 1- Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    # Double-quantize weights for extra memory savings with minimal accuracy loss\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # Use 4-bit normalized float (NF4) quantization for more accurate compression of normally distributed weights.\n",
    "    # N stands for Normalization\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# 2- Tokenizer\n",
    "\n",
    "# To create a tokenizer for Lama\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "# Use EOS as pad token to fill prompts and avoid padding warnings\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 3- The model\n",
    "\n",
    "# Loads an autoregressive (causal) LLM that predicts the next token based on previous tokens\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA,                            # Pre-trained model ID or path\n",
    "    device_map=\"auto\",                # Automatically place model layers on available devices (GPU/CPU)\n",
    "    quantization_config=quant_config  # Apply specified quantization settings for smaller memory footprint\n",
    ")\n",
    "\n",
    "# Check to how memory it uses up\n",
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDSs0rYl4t0H"
   },
   "source": [
    "## 3- Model Architecture Overview\n",
    "\n",
    "- **Embedding Layer**  \n",
    "  Converts input token IDs into dense vectors (`embed_tokens`).\n",
    "\n",
    "- **Decoder Blocks** (repeated N times)  \n",
    "  Each block contains:  \n",
    "  1. **Self-Attention** (`LlamaSdpAttention`):  \n",
    "     Tokens attend to all previous tokens (causal mask).  \n",
    "  2. **Feed-Forward Network** (`LlamaMLP`):  \n",
    "     Two linear layers with a non-linear activation in between.  \n",
    "  3. **Residual + LayerNorm** (`LlamaRMSNorm`):  \n",
    "     Stabilizes training and preserves signal via skip connections.\n",
    "\n",
    "- **Final LayerNorm**  \n",
    "  One more normalization on the last hidden state.\n",
    "\n",
    "- **Language Modeling Head** (`lm_head`)  \n",
    "  Projects hidden states back to vocabulary size to produce logits for next‐token prediction.\n",
    "\n",
    "> This stack of embedding → N×(attention + MLP + norm) → norm → head defines a causal (autoregressive) LLM, generating each token from all preceding tokens.  \n",
    "\n",
    "\n",
    "> **Tip:** Always trace the tensor shapes—ensure your vocab size, embedding dim, hidden dims, and output dim all line up.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vubHMpyh6zCj",
    "outputId": "a475d57f-c11f-419d-f877-4abdbda66556"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model is a description of the actual deep neural network that is represented by this model object\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W43b68684rkl",
    "outputId": "b3cdd818-9c8d-417a-d662-735dfaa5787f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a light-hearted joke for a room of AI Engineer<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's one:\n",
      "\n",
      "Why did the AI model go to therapy?\n",
      "\n",
      "Because it was struggling to process its emotions, but it kept getting stuck in a loop of self-reflection and couldn't \"reboot\" its feelings!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# The output: we asked for a joke. A joke for a room of AI engineer\n",
    "outputs = model.generate(inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Lj4p--Qd3v9I"
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model, inputs, tokenizer, outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGImi0a66Hs5"
   },
   "source": [
    "## 4- Streaming Results\n",
    "\n",
    "- **What?**  \n",
    "  Text streaming sends generated tokens back **as soon as** they’re produced, instead of waiting for the full sequence.\n",
    "\n",
    "- **Why?**  \n",
    "  - **Lower latency:** See words appear in real time.  \n",
    "  - **Interactive feel:** Better for chat UIs or demos.  \n",
    "  - **Early stopping:** You can stop generation mid-stream if you’ve seen enough.\n",
    "\n",
    "- **Important Notes**  \n",
    "  - Only works with models and tokenizers that support streaming.  \n",
    "  - You must pass a `streamer` argument to `model.generate()`.  \n",
    "  - Streaming adds minimal overhead—your throughput stays about the same.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t-0Sxqos6H4A"
   },
   "outputs": [],
   "source": [
    "# A function to wrap everything - with Streaming and Generation prompts\n",
    "\n",
    "def generate(llm_model, messages):\n",
    "  # Step 1 - to create a tokenizer based on the model we are working with it\n",
    "  tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token   # set the padding token to be the same as the end of sentence token\n",
    "  # Step 2 - Apply the chat template to `messages` → token IDs, move them to GPU, and store in `inputs`\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  # Step 3 - Initialize a TextStreamer to decode and stream generated tokens in real time\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  # Step 4 - Load the specified autoregressive LLM with automatic device placement and the given quantization settings\n",
    "  model = AutoModelForCausalLM.from_pretrained(llm_model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  # Step 5- Generate up to 80 new tokens, streaming each token back in real time\n",
    "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "  del inputs, model, tokenizer, outputs\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "f0f04ca584244ec899640097496f0d77",
      "107c7ccbc857457989fc7f9580d14765",
      "b463896fb9434dfe9df19e58f874494c",
      "e70e58a318d2419dbd189e4924a1e8c8",
      "5cf1e167a3c949c6a7ab5024204661de",
      "06dc36fe0e274b41b47dea7e9a2db037",
      "1f6424afe1c4402cabf1a6dd459da6dd",
      "be07f62979904cbfa44c8395b9265901",
      "b48d682b26e3485da0853e51a335ee7c",
      "a4e7f71cb9204053abe360e98504fb32",
      "9d568c4ed11c47ad92257e68a9e85f3f",
      "88f04673d82f4125815d878bd4add78c",
      "560127c666c545f8b000b84d2ecfbce0",
      "3ba06c8041a44a76a4e60d22991737f2",
      "87e350d24c5d4cda902506ab9a0bd276",
      "96010282d7864139a468b3d6b9eeba32",
      "a06617c028594ac5b4f28bd3cc727bbe",
      "a2492b691c7d49cbbf3adf05b3e3a1b9",
      "7e26bd24c51f400fb83935bee6ab777b",
      "d6c3d148270d4f1f8bffc5b2406c6843",
      "ef7404acdd0440b1a184d909cc95aded",
      "494f7262662048b4a4e9cabe161234e6",
      "99948f42f23441409ea314f599606d62",
      "e9f0ea594e82486ea0d6e24abf274de1",
      "6c86f09cbb90458bb56ab85bf4e2e545",
      "f4beaa3316ee42e8a2f1fedfb6f59472",
      "2409679a20b24ffdb06beae25bbf1199",
      "efe8c9a3aa1744ccb7d295db38f452a8",
      "0dfabc70fde94de99162bb4b661837ec",
      "30738ef5ee74450499f88a68bd24f569",
      "203c9d8ae691436fbc92658f7ca7daaf",
      "942f8d55f2d54feb87cb01d14a923110",
      "d172a4e06aa9405797b0736f42a9a6e9",
      "b20386d2ca624e7d974727b754ab2078",
      "34fd0a93066f4caf92edc6e0efc11320",
      "bc559dc673f648febc1281902b5f7908",
      "bc13fe71036b4ea38f7940f21c7d4a68",
      "8c14faef9a03424c8232caa459c6cc05",
      "6f795e8633ef4dfb918d98ecd4331429",
      "a033ef921c934ec29171a56435a05fa7",
      "2ce06fd995554f4f90ddd222d1e43dc5",
      "7ec747f9b75e404685109f638afd8ba2",
      "5e489413bbfd416c847cc2c106bbe9dd",
      "3e3cb3e4b8ae4ab3a757dcab0be53a13",
      "7060213be3fd4989bc242ac9e849b2ee",
      "cf6310eadac245f9ab1f0a6e94bf370a",
      "6890bfa8389b4da684cdabdc4b2db258",
      "d1bd28eb8a904effba31eb1c1651d5e1",
      "249ac6bd8e17499d9e3f4f69253e946a",
      "fd2e75c3844740a4865b7d6a9a9f3e5f",
      "1f5b6e98011345ef9a9d6dc5862e72f8",
      "ab2c90513f414e20874d1cb5e87e4a1f",
      "db560914f7e94a6fbcc6472814512d6f",
      "404b4f59fe894d1b85d7363c1500a4b1",
      "4ef2e001a4344c2c95ca80e318a948c4",
      "bed2c026de264e869d0522bc7ac23e18",
      "1b8918e66aa548aba55fb8406865c5d1",
      "1fc115acd8674fa186db1f6de820ed4d",
      "8e51b5d53de54225b11f7e9074ca38ff",
      "249cb8e6b1334d4db0842df0e46cec0f",
      "45ebcb88ca684005987a2430567d3658",
      "663567a6bf984e0696dec1676c973db9",
      "2d91f2a5b24e41f5b7e87a2edf2455ec",
      "b7b10cad2b2048feb8a07b7d39eecc8f",
      "439a513c7be44240b76466799d35241a",
      "fe026f9c76014e4bbb173bf572775a36",
      "351881b487fe4fc79e366d8bb8cf97f9",
      "d92dc27e58c04afd828afb1a0b4399d9",
      "31f13d06f6f0487f9147aba3e43d5712",
      "f65da3dfdc2a4daf9b19bfd48bf48989",
      "9a0663f80ba846268cd3468e722bbf28",
      "88580e022c684a4b92bd57457a52814b",
      "d56ce99d35ff438ea37ef361675fa994",
      "733e6b0a7380419f87282401cf88a499",
      "c6cdd9fb97d946f9bde5a86d9eab7b2b",
      "17b14eccd6ca4e789ddf8721a0fdc56e",
      "b822983e3b04466d848f0467aac3717f",
      "1f8b392689cb4053ab1e0f5fe40b7631",
      "f9b9201c60d64e70ba38ffd94c38b36b",
      "1a2e0acf4d9c41baadbfc86f393db3b0",
      "68e4b489b9c54594838c7d1aeda488db",
      "c887ece13d6b4965a2bdbbc33aad37e8",
      "1fff4ee0e08c406898de6552f3f30984",
      "2de3c086ac424d5ca9828fae7a3cbcc9",
      "b8dae3b9af76477cbf92d1c5bb091c21",
      "1ae21486ae7242c582e07357076c6c06",
      "54417eea71924d1389108f88987f871a",
      "b09bf4fb7d944384b8e16fea49d05c09",
      "af40dd6257d44530b480abc504968119",
      "477eaa91518a4c3d99ec612a8ff48fa9",
      "1cebbbd81f554e738a9f3722bcb31dcd",
      "3d447a530b144e42912407cc617339b6",
      "39e1fea34a7346d6a6d32e8460a3fc05",
      "5d530cd5fac8496eb07cd72abb5a93d0",
      "474e033cde234a2599616f16355739ef",
      "2aaaab146c0f43e3a62761661dec091f",
      "5b439938712c44cd9c070cba6fccb67e",
      "012ed52373024af4b11cf621583d37b1",
      "a9652846e6f542e98d6d59ec93f7b74b",
      "c68d8f4b439142ca92954dfe2b15e8fb",
      "ffd27ceda3ed46f38f24831484d4003d",
      "bdc626489a104b39bf0fe7405ba1e9a4",
      "88de5c99855e4aa691ef602705cefb75",
      "842b2000afa343b3bb1b4f2edf1c9072",
      "d217b31d86c44a24ac6de0912ddb8d5b",
      "b30c1824ccbc40278c2a79bee7bafa41",
      "472f1421fe03421cb3c7f8ff798375c5",
      "0a0bc9851e6d4d7db79d5bec61924d2f",
      "1bf06eb7849841c1a2f2f554bb5f9422",
      "0299d9b76c82465ea2afe7cb967c2cd6",
      "ff6df9d348e2453aa078c224bc05dae6",
      "88d6a6a28a6342718bc22c42c9e8318b",
      "d27234981bea4d6dad850fd6ddf21346",
      "880596fb8639457bb819998d7b59f697",
      "b2d13965268a40bfb80cf29de2dca403",
      "c8777a6f34a945cb80548adaad60cb94",
      "0dcf227664a541fa9cbe723362778b1a",
      "01b674c6a3494222b93458f90c9578b5",
      "c36d992571754da8b118d43bbd59331e",
      "4701f483f0284ab990d39d0cd2c73951",
      "c7da8b421a8646bfa6f0812e523062a6",
      "3f66258b2572473a963f5447ac8930ba",
      "1a809b8ee3734d83837c693c3697664c",
      "4b8201621b614b1c877ef989710c4562",
      "49d498f6aeee485faaeb1dbf1b465f65",
      "387c7ed8bab043adb1277c4832b20a18",
      "9a391ea0856748e29ac5a21886635686",
      "b7539fc74ae3465da78a4c38e1fb941f",
      "104c039a75514dbf9c030a373fb2c51d",
      "189d3a3a255d4855808542d5b31d1733",
      "90a38166d9e549f1a026529924e9503c",
      "5e489fd7d7a1426f96a428eae31b6984"
     ]
    },
    "id": "Snmjq0S8-9Cf",
    "outputId": "922ec762-7cd1-437c-a2d7-cf4d09cfab4b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f04ca584244ec899640097496f0d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f04673d82f4125815d878bd4add78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99948f42f23441409ea314f599606d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20386d2ca624e7d974727b754ab2078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7060213be3fd4989bc242ac9e849b2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed2c026de264e869d0522bc7ac23e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351881b487fe4fc79e366d8bb8cf97f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8b392689cb4053ab1e0f5fe40b7631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af40dd6257d44530b480abc504968119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68d8f4b439142ca92954dfe2b15e8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6df9d348e2453aa078c224bc05dae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f66258b2572473a963f5447ac8930ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful assistant<|end|><|user|> Tell a light-hearted joke for a room of AI Engineer<|end|><|assistant|> Why don't AI Engineers ever play hide and seek?\n",
      "\n",
      "Because good luck hiding when they're always so good at finding you!<|end|>\n"
     ]
    }
   ],
   "source": [
    "generate(PHI3, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603,
     "referenced_widgets": [
      "651b39c96378402db23ee634f68abd27",
      "574f96fb26da4b2bb74b5f882072eeb8",
      "909c6534de5748729dc122297b07d44e",
      "81435cb9058f477fa783d563c0582e31",
      "11433a43299d4543a20e39d89dfe4d3f",
      "7c983ff0789240bcac16034d69dd1d0d",
      "ea16769452294d41b3e36f13a2a00dd5",
      "9424edd97c334b30b01b02ec5b6871f9",
      "e76b88ebbb154305a84762b15e473995",
      "d1c01d85e9c84cafab3f11505024bdaf",
      "b3247901f73f4ae1a4c1f1c0095ad4de",
      "1815c9a401f14c7aab2108d08af3c936",
      "69e0b9178e4444c6813f03c02728ab44",
      "7ab5984ef72c4c2796f621ca8e12b4ea",
      "6722c4b9d1b5461097ba9a1c3e7b9078",
      "a9f4565fd67f48ce81cf35a0a0dbe602",
      "5a4b21c9dddb4fa091e607298fada80f",
      "dd752cdb4af947ab9257b54933709308",
      "4b96e8d64ff14eee969f02db675b4f85",
      "2a5a7d9929284148b5804807bd783b8b",
      "43538a8984704a9da4c35749bd8d0958",
      "9d95ceabd82b4fd78956d13b39a9790d",
      "537b9bd3b2ad4722a9cdbce6d0c55e25",
      "38ea1c3e0e91488dbe843ebd772627fd",
      "738bb115abdd4570ac91bc06c2778f61",
      "9469ce8b1ce34f4a89578de81482f15d",
      "f798e208b34a4723bb9762ecf51f5b27",
      "2b772f3602054c38a56dfeceba2efb0a",
      "4fe14a5187e747e6a116fb561e819ea5",
      "5c4b0f2a537440d29a77c29a90af4324",
      "047cab5b899c4ea98676ddba5e3cdbf5",
      "a825ca21cebf4290baf4fc585f898a88",
      "3aa1d1304d3042329b414dbfd22615b8",
      "c6df2e8523f14426945780a13899402d",
      "99d962168d3e4ece911e1db7fa7458e0",
      "ff2d8c285c6b4b75a5cc471ff1427bb8",
      "0f7eabe78a0644ae92a6b74aab775374",
      "d11cb38a29d14939aed9016d760469bf",
      "da4bccb461ae49948effa2e462d2b2a8",
      "c396ec107695480ebf6e1258ff43f812",
      "fff8a073c2ed4cd38a2fafa1bd2f6b45",
      "8593b12885c048c184d019708757a0c4",
      "99477326202249d69e9947f6b9b7e5c0",
      "b99276142e254d24bd5e0aa9dc3eae6c",
      "a32163bc37c4428da5641b6c99eca3fc",
      "c08856ba5fef43dfb2a68dabf40dc03f",
      "2cc90f9f51a6458985b4a8512ff28267",
      "bd1701390e5e48ce900ca22fda069e92",
      "9cdcbc11f4c8442c9d90a5c2520ddcca",
      "5f58143b50bf4d9dafbd4011a7d9b866",
      "f62663da501042cd942e6c010736fcbc",
      "917c4dd020a0437aa82b3ded20dacf81",
      "322a0f0f08494315b6ac86818306f4d3",
      "5bb8731cdef947bc8862b62d5ec23db7",
      "db1a42b672c34d748129375c82fe273c",
      "c4dc7774a03f43b98766411e8311148c",
      "014c1f543839473baf50484f06a2c59c",
      "d670ba37598b4d4a9e5644ef4975d7b8",
      "1734ae31f9c14ec89571953674880e4b",
      "022f5abc61364706827d7f650d00d5da",
      "b1bdd05c9ab449aa8702f6c25c599bc3",
      "b10d59f096454e4f904eacaae4d00749",
      "4ff77a290f9c4244bd46ebc68492a0d5",
      "f66427699574487a9f8a8e6d4aee43a6",
      "7fea37f11387465d90e06b637b1ffc1e",
      "a50a6d20b42b40b587b562d1ee9a8989",
      "87350db23662460cb57a91316533967c",
      "13e286bdc8c34bfbad58771d7956db9b",
      "b255fa2c60e74c08962b5edf0ccef92a",
      "d187463dc4e94fe296ecb90e6c432cf9",
      "e51ab17a0ce04be7821118cece6bb20f",
      "d2996b91d3b2485f811492d4feb24942",
      "2e0d58d9ee914f5bb97de192cfe1ccb3",
      "5ab3d6e4fc7d430183997b7d4bab4a37",
      "7073647fce74453dbc92cce868aea72e",
      "359c918037884ea89ae67b4fb719b52c",
      "e808986598d342f294a12e7afbf93c39",
      "3d5ca2c06b3249a1851054487524c8b7",
      "f4a39d0429684af18f5659bb5a2cfc30",
      "c27a1a72d4e245868eb9191d8b088282",
      "b35217bf7751432397118fc53414eb46",
      "c3b1ce53144c4524a80f37dd057c00a5",
      "f90d541cefdf4cc0babdfcc61c16c465",
      "0b7a7fe5ec75442999bb7a4fc38b7202",
      "c0452e18035846af862f3d8e2709391d",
      "fa31dccba0434a17810e935fd4b51511",
      "0d92c0e1392c44ce9df64674864e2d0c",
      "789b1f49f870448ca4f56c52f4cc97ad",
      "15e934b3790e43f5a16c777c15fae2fc",
      "e21ec255ae0f41fb8043533c3e874bab",
      "31d0ab2d883c4555b156706e5e1d3933",
      "c9a0887eccbf40cdb99ef13887f84e0b",
      "71a25dbed9854b6d8c821b9f1bce2144",
      "74d8c2604e764509a068d197b9772410",
      "bca5206a833840d2ae6497f2a9508d30",
      "3c24c5790dfa4cb39c58ec377e2440e1",
      "d1184ae80e5248a78da65777edc05b42",
      "f9b568281a514e8e9b5aaf9a1e8e935b",
      "c8c698a265c54550987cb0448bb35e42",
      "623686542e85460aa2c9542204e553e4",
      "af96858cc86c495abc79ecc4892456a1",
      "ddf89239b169409ca254d2f991bf2fb3",
      "b8d967797bcc4c05b227ff1fb454b19f",
      "0c7c0a7e6b804c5cbb2475e046b8a012",
      "882ac09416274f1cbe79a1b7fb8eda80",
      "d768ff3f39154ba9bac54ac3fe2601c2",
      "86347c6573684764b84ce33b4996e324",
      "66191e68d30d46fd97bfef6420c6a95b",
      "a9fbc65fd76543379fd5ca6e7c8f0249",
      "e40bcc3e5360440297e234678e5a4198",
      "587b86d7cf6c44df829195c1c0cbcdda",
      "a3fd195c817b4463ac66da373fc48b61",
      "e061f17e805144d88171612dd23f08cb",
      "0c23277de01d4d46aa7712a098ae117a",
      "1062a79f77bc442c91df4b497210961c",
      "ceff2d1ebe1e4261836e6beee496fd41",
      "53ff256a500b4235ac44b142df039a73",
      "6a637512042d4ae3b30ca3bf1aa3f35e",
      "ac39e3acb0554fcea9c5e5bee4712321",
      "e8c5a24664da458c9051e1d953b400d0",
      "793f43f236b54f129290e3edc31bf79e"
     ]
    },
    "id": "14KXftgL_BNB",
    "outputId": "db0f8a82-74ce-423e-942d-07a8fdf45e32"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651b39c96378402db23ee634f68abd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1815c9a401f14c7aab2108d08af3c936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537b9bd3b2ad4722a9cdbce6d0c55e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6df2e8523f14426945780a13899402d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32163bc37c4428da5641b6c99eca3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dc7774a03f43b98766411e8311148c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87350db23662460cb57a91316533967c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5ca2c06b3249a1851054487524c8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e934b3790e43f5a16c777c15fae2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623686542e85460aa2c9542204e553e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587b86d7cf6c44df829195c1c0cbcdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell a light-hearted joke for a room of AI Engineer<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the AI confused at the party? \n",
      "\n",
      "Because it couldn't find its *training data*! 😂 \n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like to hear another joke! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# NOTE: to access Gemma from google, we need to accept their terms in huggingface: https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of AI Engineer\"}\n",
    "]\n",
    "\n",
    "generate(GEMMA2, messages)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
