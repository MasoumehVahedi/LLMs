{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview\n",
        "\n",
        "### Problem\n",
        "- Meetings generate **long audio recordings** that few people re-watch.  \n",
        "- Manual note-taking is slow and often misses **actions, owners, and deadlines**.  \n",
        "- Teams lose accountability and spend time asking “Who’s doing what?”\n",
        "\n",
        "### Goal\n",
        "Create an automated tool that turns any meeting recording into concise, shareable **minutes** that include:\n",
        "1. Agenda / summary  \n",
        "2. Discussion points & take-aways  \n",
        "3. Action items with clear owners  \n",
        "\n",
        "### Solution Overview\n",
        "1. **Audio → Text**  \n",
        "   - Send the recording to a *frontier* speech-to-text API (e.g., OpenAI Whisper) to get a raw transcript.\n",
        "2. **Text → Minutes**  \n",
        "   - Feed the transcript to an *open-source* LLM that:\n",
        "     - Summarises the meeting  \n",
        "     - Extracts actions, owners, and deadlines\n",
        "3. **Streaming Output**  \n",
        "   - Stream the generated minutes back in real time and display them in Markdown.\n",
        "\n",
        "Result: fully-formatted meeting minutes delivered within minutes of the call—no human note-taker required.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Dataset\n",
        "\n",
        "I will use the **MeetingBank_Audio** dataset, which contains audios:\n",
        "\n",
        "> https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main\n",
        "\n",
        "I download \"Boston\" audio.\n"
      ],
      "metadata": {
        "id": "2SYQjDPGdgGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2EEgdck6VmR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2945053-b473-4371-c5f3-3455619c93c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq bitsandbytes>=0.43.0 accelerate>=0.27.0 transformers>=4.41.0"
      ],
      "metadata": {
        "id": "jYTKmaQf6rOF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import requests\n",
        "import argparse\n",
        "import zipfile, pathlib\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from typing import Callable, Optional, List\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor"
      ],
      "metadata": {
        "id": "hivUq2xsdg8y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Helper class\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class MeetingMinutesGenerator:\n",
        "  \"\"\"End‑to‑end audio → minutes generator.\"\"\"\n",
        "  def __init__(self,\n",
        "               # ASR\n",
        "               asr_backend: str = \"openai\",\n",
        "               audio_model: str = \"whisper-1\",\n",
        "               openai_api_key: Optional[str] = \"None\",\n",
        "               hf_whisper_checkpoint: str = \"openai/whisper-medium\",\n",
        "               # Summariser\n",
        "               llm_model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "               hf_token: Optional[str] = \"None\",\n",
        "               load_in_4bit: bool = True,\n",
        "               streaming: bool = True,\n",
        "               device: str = \"auto\",\n",
        "               # Callback for streamed text (get tokens as they arrive)\n",
        "               stream_callback: Optional[Callable[[str], None]] = \"None\"\n",
        "               ):\n",
        "    self.asr_backend = asr_backend\n",
        "    self.audio_model = audio_model\n",
        "    self.openai_api_key = openai_api_key or userdata.get('OPENAI_API_KEY')   # not colab: os.getenv(\"OPENAI_API_KEY\")\n",
        "    self.hf_whisper_checkpoint = hf_whisper_checkpoint\n",
        "\n",
        "    self.llm_model = llm_model\n",
        "    self.hf_token = hf_token or userdata.get(\"HF_TOKEN\")   # not colab: os.getenv(\"HF_TOKEN\")\n",
        "    self.load_in_4bit = load_in_4bit\n",
        "    self.streaming = streaming\n",
        "    self.device = device\n",
        "    self.stream_callback = stream_callback\n",
        "\n",
        "    # Sign in to OpenAI using Secrets in Colab\n",
        "    if self.asr_backend == \"openai\":\n",
        "      if not self.openai_api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY not set\")\n",
        "      self.openai_client = OpenAI(api_key=self.openai_api_key)\n",
        "\n",
        "    # If you asked for 4-bit weights, build a BitsAndBytesConfig…\n",
        "    if self.load_in_4bit:\n",
        "      quant_config = BitsAndBytesConfig(\n",
        "          load_in_4bit=True,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "          bnb_4bit_quant_type=\"n4f\"\n",
        "      )\n",
        "    else:\n",
        "      quant_config = None\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model, token=self.hf_token)  # login() and the token= argument do the same job\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "        self.llm_model,\n",
        "        device_map=self.device,\n",
        "        quantization_config=quant_config,\n",
        "        token=self.hf_token\n",
        "        )\n",
        "    # choose streamer\n",
        "    if self.streaming:\n",
        "      self.streamer = TextStreamer(self.tokenizer)\n",
        "    else:\n",
        "      self.streamer = None\n",
        "\n",
        "\n",
        "  # ASR helpers\n",
        "  def loadWhisper(self) -> None:\n",
        "    \"\"\"Load open‑source Whisper as an HF pipeline for local ASR.\"\"\"\n",
        "    speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        self.hf_whisper_checkpoint,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        use_safetensors=True,\n",
        "        token=self.hf_token\n",
        "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(self.hf_whisper_checkpoint, token=self.hf_token)\n",
        "\n",
        "    self.whisper_pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=speech_model,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        feature_extractor=processor.feature_extractor,\n",
        "        torch_dtype=torch.float16,\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "\n",
        "  def transcribe(self, audio_path: str) -> str:\n",
        "    \"\"\"Return text transcription of the audio file.\"\"\"\n",
        "    if self.asr_backend == \"openai\":\n",
        "      with open(audio_path, \"rb\") as audio_file:\n",
        "        resp = openai.audio.transcription.create(\n",
        "            model=self.audio_model,\n",
        "            file=audio_file,\n",
        "            response_format=\"text\"\n",
        "        )\n",
        "      return resp\n",
        "\n",
        "    # Use the Whisper OpenAI model to convert the Audio to Text\n",
        "    result = self.whisper_pipe(audio_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "\n",
        "  # Summarisation prompt helpers\n",
        "  PROMPT_TEMPLATE = (\n",
        "        \"You are an assistant that produces clear meeting minutes.\\n\"\n",
        "        \"Return Markdown containing:**\\n\\n\"\n",
        "        \"**Date & Location**\\n\"\n",
        "        \"**Attendees**\\n\"\n",
        "        \"**Summary**\\n\"\n",
        "        \"**Discussion Points** (bullet list)\\n\"\n",
        "        \"**Take‑aways** (bullet list)\\n\"\n",
        "        \"**Action Items** (task, owner, deadline)\\n\\n\"\n",
        "        \"Transcript:\\n{transcript}\\n\"\n",
        "    )\n",
        "\n",
        "  def makeMessages(self, transcript: str) -> List[dict]:\n",
        "    system_msg = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant that writes structured meeting minutes in Markdown.\"\n",
        "        }\n",
        "    user_msg = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": self.PROMPT_TEMPLATE.format(transcript=transcript),\n",
        "        }\n",
        "\n",
        "    return [system_msg, user_msg]\n",
        "\n",
        "\n",
        "  def summarize(self, transcript: str, max_new_tokens: int = 2048) -> str:\n",
        "    \"\"\"Return Markdown minutes for the given transcript.\"\"\"\n",
        "    messages = self.makeMessages(transcript)\n",
        "\n",
        "    inputs = self.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        add_generation_prompt=True\n",
        "    ).to(self.model.device)\n",
        "\n",
        "    gen_kwargs = {\n",
        "        \"inputs\": inputs,\n",
        "        \"max_new_tokens\": max_new_tokens\n",
        "    }\n",
        "    if self.streamer is not None:\n",
        "      gen_kwargs[\"streamer\"] = self.streamer\n",
        "\n",
        "    output_ids = self.model.generate(**gen_kwargs)\n",
        "\n",
        "    # if streaming, TextStreamer already prints; but we still return full text\n",
        "    return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "  def run(self, audio_path: str) -> str:\n",
        "    \"\"\"Full pipeline: audio → minutes (Markdown).\"\"\"\n",
        "    transcript = self.transcribe(audio_path)\n",
        "    minutes_md = self.summarize(transcript)\n",
        "    if self.stream_callback:\n",
        "      self.stream_callback(minutes_md)\n",
        "    return minutes_md"
      ],
      "metadata": {
        "id": "JwKQB7hLdg_U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "302GP4rbyWN_",
        "outputId": "34def023-36b0-484c-e81c-31fd042bc63e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "  zip_path = \"/content/drive/MyDrive/Boston.zip\"\n",
        "  target_dir = pathlib.Path(\"/content/audio\")\n",
        "  with zipfile.ZipFile(zip_path) as zf:\n",
        "      zf.extractall(target_dir)\n",
        "  audio_file = sorted(target_dir.rglob(\"*.mp3\"))[0]   # first MP3\n",
        "\n",
        "  # 3- Pull secrets if running in Colab\n",
        "  OPENAI_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "  HF_TOKEN   = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "  # 4- Instantiate the helper\n",
        "  mm = MeetingMinutesGenerator(\n",
        "      asr_backend=\"openai\",\n",
        "      openai_api_key=OPENAI_KEY,\n",
        "      hf_token=HF_TOKEN,\n",
        "      llm_model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "      load_in_4bit=True,\n",
        "  )\n",
        "\n",
        "  # 5- Run end-to-end\n",
        "  markdown_minutes = mm.run(str(audio_file))\n",
        "\n",
        "  print(\"\\n\\nGenerated Minutes\\n-----------------\\n\")\n",
        "  print(markdown_minutes)"
      ],
      "metadata": {
        "id": "jR09YnGVdhBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}