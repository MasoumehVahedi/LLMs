### Key LLM Leaderboards

1. **Hugging Face Open-LLM Leaderboard**  
   Automatic benchmark scores (MMLU, GSM-8K, TruthfulQA, …) for general-purpose open models.  
   <https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>

2. **Hugging Face BigCode Leaderboard**  
   HumanEval / MultiPL-E results and throughput for code-generation models such as StarCoder 2 and CodeLlama.  
   <https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard>

3. **Hugging Face LLM-Perf**  
   Reports speed, memory use and energy on real GPUs/CPUs—helps you check if a model fits your hardware budget.  
   <https://huggingface.co/spaces/optimum/llm-perf-leaderboard>

4. **Hugging Face Domain-Specific Leaderboards**  
   Community spaces that track specialised tasks (medical, legal, multilingual, etc.).  
   Example hub: <https://huggingface.co/spaces?task=leaderboard>

5. **Vellum Leaderboard**  
   Business-oriented sheet showing context window, latency and \$-per-million-tokens for both closed and open models.  
   <https://www.vellum.ai/llm-leaderboard>

6. **SEAL Leaderboards (Scale AI)**  
   Expert-graded evaluations on coding, maths and instruction-following—human scores instead of pure benchmarks.  
   <https://scale.com/leaderboard>

